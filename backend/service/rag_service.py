"""
RAG (Retrieval-Augmented Generation) ì„œë¹„ìŠ¤
ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ AI ì‘ë‹µ ìƒì„±
"""
from typing import List, Dict
from openai import AzureOpenAI
import os
from dotenv import load_dotenv

from service.azure_search import AzureSearchService
from service.query_preprocessor import QueryPreprocessor

load_dotenv()


class RAGService:
    """RAG ì„œë¹„ìŠ¤ - ê²€ìƒ‰ + ìƒì„±"""
    
    def __init__(self):
        # OpenAI í´ë¼ì´ì–¸íŠ¸
        self.openai_client = AzureOpenAI(
            api_key=os.getenv("AZURE_OPENAI_API_KEY"),
            api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT")
        )
        
        # Search ì„œë¹„ìŠ¤
        self.search_service = AzureSearchService()
        
        # â­ ì¿¼ë¦¬ ì „ì²˜ë¦¬ê¸° ì¶”ê°€
        self.preprocessor = QueryPreprocessor()
        
        print("âœ… RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def get_embedding(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜"""
        try:
            response = self.openai_client.embeddings.create(
                model=os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT"),
                input=text
            )
            return response.data[0].embedding
        except Exception as e:
            print(f"âš ï¸ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return [0.0] * 1536
    
    def search_relevant_documents(
        self,
        query: str,
        top: int = 5,
        min_score: float = 0.5  # â­ 0.7ì—ì„œ 0.5ë¡œ ë‚®ì¶¤
    ) -> List[Dict]:
        """
        ì¿¼ë¦¬ì— ê´€ë ¨ëœ ë¬¸ì„œ ê²€ìƒ‰
        """
        # ì¿¼ë¦¬ ì „ì²˜ë¦¬ (í•œê¸€ â†’ ì˜ì–´ ë³€í™˜)
        processed_query = self.preprocessor.preprocess(query)
        
        if processed_query != query:
            print(f"ğŸ”„ ì¿¼ë¦¬ ì „ì²˜ë¦¬: '{query}' â†’ '{processed_query}'")
        else:
            print(f"ğŸ” ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©: '{query}'")
        
        # ì „ì²˜ë¦¬ëœ ì¿¼ë¦¬ë¡œ ë²¡í„° ìƒì„±
        query_vector = self.get_embedding(processed_query)
        
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰
        documents = self.search_service.hybrid_search(
            query=processed_query,
            query_vector=query_vector,
            top=top
        )
        
        # â­ ê²€ìƒ‰ëœ ëª¨ë“  ë¬¸ì„œì˜ ì ìˆ˜ ì¶œë ¥
        print(f"ğŸ“Š ê²€ìƒ‰ëœ ë¬¸ì„œ: {len(documents)}ê°œ")
        for i, doc in enumerate(documents[:3], 1):
            print(f"  {i}. [{doc.get('source', 'unknown')}] ì ìˆ˜: {doc.get('score', 0):.4f} - {doc.get('title', 'No title')[:50]}")
        
        # ì ìˆ˜ í•„í„°ë§
        filtered_docs = [
            doc for doc in documents 
            if doc.get('score', 0) >= min_score
        ]
        
        if filtered_docs:
            print(f"âœ… í•„í„°ë§ ê²°ê³¼: {len(documents)}ê°œ â†’ {len(filtered_docs)}ê°œ (ìµœì†Œ ì ìˆ˜: {min_score})")
        else:
            print(f"âš ï¸ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ ì—†ìŒ (ëª¨ë“  ë¬¸ì„œê°€ {min_score} ì´í•˜)")
            if documents:
                max_score = max([doc.get('score', 0) for doc in documents])
                print(f"   ìµœê³  ì ìˆ˜: {max_score:.4f}")
        
        return filtered_docs
    
    def generate_context_aware_summary(
        self,
        query: str,
        documents: List[Dict]
    ) -> str:
        """
        ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§¥ë½ ìš”ì•½ ìƒì„±
        """
        # â­ ë¬¸ì„œê°€ ì—†ê±°ë‚˜ ê´€ë ¨ì„±ì´ ë‚®ì€ ê²½ìš°
        if not documents:
            return self._generate_no_result_message(query)
        
        # ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        context = "\n\n".join([
            f"[{doc['source']}] {doc['title']}\nì‘ì„±ì¼: {doc['date']}\në‚´ìš©: {doc['content'][:500]}"
            for doc in documents[:3]
        ])
        
        # â­ AIì—ê²Œ ê´€ë ¨ì„± ê²€ì¦ ìš”ì²­
        system_prompt = """ë‹¹ì‹ ì€ ì—…ë¬´ ë§¥ë½ì„ ë¶„ì„í•˜ëŠ” AI ë„ìš°ë¯¸ì…ë‹ˆë‹¤.

ì¤‘ìš”: ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ì œê³µëœ ë¬¸ì„œê°€ ê´€ë ¨ì´ ì—†ë‹¤ë©´, ì†”ì§í•˜ê²Œ "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
ë¬¸ì„œì˜ ë‚´ìš©ì„ ì–µì§€ë¡œ ì§ˆë¬¸ì— ë§ì¶”ë ¤ í•˜ì§€ ë§ˆì„¸ìš”.

ë§Œì•½ ì§ˆë¬¸ê³¼ ë¬¸ì„œê°€ ê´€ë ¨ì´ ìˆë‹¤ë©´, ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:

## ğŸ“‹ ì—…ë¬´ ë§¥ë½ ë¶„ì„ ê²°ê³¼

### ğŸ¯ í•µì‹¬ ìš”ì•½
(í•œ ë¬¸ì¥ìœ¼ë¡œ ì—…ë¬´ì˜ ë³¸ì§ˆ ì„¤ëª…)

### ğŸ“– ë°°ê²½ ë° ëª©ì 
(ì™œ ì´ ì—…ë¬´ê°€ ìƒê²¼ëŠ”ì§€)

### ğŸ‘¥ ì£¼ìš” ì´í•´ê´€ê³„ì
(ê´€ë ¨ëœ ì‚¬ëŒë“¤ê³¼ ì—­í• )

### ğŸ“… ì¼ì • ë° ìš°ì„ ìˆœìœ„
(ë§ˆê°ì¼, ìš°ì„ ìˆœìœ„)

### ğŸ”‘ í•µì‹¬ ë‚´ìš©
(ê¸°ìˆ ì  ìš”êµ¬ì‚¬í•­ì´ë‚˜ ì¤‘ìš” ì‚¬í•­)

ê°„ê²°í•˜ê³  êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."""

        user_prompt = f"""
ì‚¬ìš©ì ì§ˆë¬¸: {query}

ê´€ë ¨ ë¬¸ì„œ:
{context}

ìœ„ ë¬¸ì„œë“¤ì´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆë‚˜ìš”? 
ê´€ë ¨ì´ ìˆë‹¤ë©´ ìœ„ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ê³ , ê´€ë ¨ì´ ì—†ë‹¤ë©´ ì†”ì§í•˜ê²Œ "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•´ì£¼ì„¸ìš”.
"""
        
        try:
            response = self.openai_client.chat.completions.create(
                model=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME"),
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.7,
                max_tokens=1000
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            print(f"âŒ AI ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return "AI ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def _generate_no_result_message(self, query: str) -> str:
        """ê´€ë ¨ ë¬¸ì„œê°€ ì—†ì„ ë•Œ ë©”ì‹œì§€"""
        return f"""## ğŸ” ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ

ì£„ì†¡í•©ë‹ˆë‹¤. **"{query}"**ì™€ ê´€ë ¨ëœ ì—…ë¬´ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

### ğŸ’¡ ë„ì›€ë§

ë‹¤ìŒê³¼ ê°™ì´ ì‹œë„í•´ë³´ì„¸ìš”:

1. **êµ¬ì²´ì ì¸ í‚¤ì›Œë“œ ì‚¬ìš©**
   - ì˜ˆ: "Redis Stream", "í…Œì´ë¸” ì„¤ê³„", "CHUB í”„ë¡œì íŠ¸"

2. **ì—…ë¬´ ê´€ë ¨ ì§ˆë¬¸**
   - ì˜ˆ: "â—‹â—‹ í”„ë¡œì íŠ¸ ì¼ì •ì´ ì–¸ì œì•¼?"
   - ì˜ˆ: "â–³â–³ ë‹´ë‹¹ìê°€ ëˆ„êµ¬ì•¼?"

3. **ë¬¸ì„œê°€ ì—…ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸**
   - Blob Storageì— ê´€ë ¨ ë¬¸ì„œê°€ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”

í˜„ì¬ ì‹œìŠ¤í…œì— ì—…ë¡œë“œëœ ë¬¸ì„œ ë‚´ì—ì„œë§Œ ê²€ìƒ‰ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤."""
    
    def generate_action_items(
        self,
        query: str,
        documents: List[Dict]
    ) -> List[str]:
        """ì•¡ì…˜ ì•„ì´í…œ ìƒì„±"""
        if not documents:
            return ["ê´€ë ¨ ë¬¸ì„œê°€ ì—†ì–´ ì•¡ì…˜ ì•„ì´í…œì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."]
        
        context = "\n".join([
            f"[{doc['source']}] {doc['content'][:300]}"
            for doc in documents[:2]
        ])
        
        prompt = f"""
ë‹¤ìŒ ì—…ë¬´ ìƒí™©ì—ì„œ í•´ì•¼ í•  êµ¬ì²´ì ì¸ ì•¡ì…˜ ì•„ì´í…œì„ 3-4ê°œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {query}

ê´€ë ¨ ì •ë³´:
{context}

ê° í•­ëª©ì€ í•œ ì¤„ë¡œ ì‘ì„±í•˜ê³ , "- "ë¡œ ì‹œì‘í•´ì£¼ì„¸ìš”.
"""
        
        try:
            response = self.openai_client.chat.completions.create(
                model=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME"),
                messages=[
                    {"role": "system", "content": "ì•¡ì…˜ ì•„ì´í…œì„ ëª…í™•í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•˜ê²Œ ì‘ì„±í•©ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.5,
                max_tokens=400
            )
            
            items = response.choices[0].message.content.strip().split('\n')
            return [item.strip('- ').strip() for item in items if item.strip() and item.strip().startswith('-')]
            
        except Exception as e:
            print(f"âŒ ì•¡ì…˜ ì•„ì´í…œ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return ["ì•¡ì…˜ ì•„ì´í…œ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."]